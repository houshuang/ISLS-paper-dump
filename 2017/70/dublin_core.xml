<?xml version="1.0" encoding="utf-8" standalone="no"?>
    <dublin_core schema="dc">
    <dcvalue element="contributor" qualifier="author">Eagan, Brendan R.</dcvalue>
<dcvalue element="contributor" qualifier="author">Rogers, Bradley</dcvalue>
<dcvalue element="contributor" qualifier="author">Serlin, Ronald</dcvalue>
<dcvalue element="contributor" qualifier="author">Ruis, Andrew R.</dcvalue>
<dcvalue element="contributor" qualifier="author">Irgens, Golnaz Arastoopour</dcvalue>
<dcvalue element="contributor" qualifier="author">Shaffer, David Williamson</dcvalue>
      <dcvalue element="date" qualifier="accessioned">2017-06-19T10:52:18Z</dcvalue>
      <dcvalue element="date" qualifier="available">2017-06-19T10:52:18Z</dcvalue>
      <dcvalue element="date" qualifier="issued">2017-07</dcvalue>
      <dcvalue element="identifier" qualifier="citation" language="en_US">Eagan, B. R., Rogers, B., Serlin, R., Ruis, A. R., Irgens, G. A., &amp; Shaffer, D. W.&#x20;(2017).&#x20;Can We Rely on IRR? Testing the Assumptions of Inter-Rater Reliability&#x20;In&#x20;Smith,&#x20;B.&#x20;K.,&#x20;Borge,&#x20;M.,&#x20;Mercier,&#x20;E.,&#x20;and&#x20;Lim,&#x20;K.&#x20;Y.&#x20;(Eds.).&#x20;(2017).&#x20;Making&#x20;a&#x20;Difference:&#x20;Prioritizing&#x20;Equity&#x20;and&#x20;Access&#x20;in&#x20;CSCL,&#x20;12th&#x20;International&#x20;Conference&#x20;on&#x20;Computer&#x20;Supported&#x20;Collaborative&#x20;Learning&#x20;(CSCL)&#x20;2017,&#x20;Volume&#x20;2.&#x20;Philadelphia,&#x20;PA:&#x20;International&#x20;Society&#x20;of&#x20;the&#x20;Learning&#x20;Sciences.</dcvalue>
      <dcvalue element="identifier" qualifier="uri">https:dx.doi.org&#x2F;10.22318&#x2F;cscl2017.70</dcvalue>
      <dcvalue element="description" qualifier="abstract" language="en_US">Researchers use Inter-Rater Reliability (IRR) to measure whether two processes—people and/or machines—identify the same properties in data. There are many IRR measures, but regardless of the measure used, however, there is a common method for estimating IRR. To assess the validity of this common method, we conducted Monte Carlo simulation studies examining the most widely used measure of IRR: Cohen’s kappa. Our results show that the method commonly used by researchers to assess IRR produces unacceptable Type I error rates.</dcvalue>
      <dcvalue element="language" qualifier="iso" language="en_US">en</dcvalue>
      <dcvalue element="publisher" qualifier="none" language="en_US">Philadelphia,&#x20;PA:&#x20;International&#x20;Society&#x20;of&#x20;the&#x20;Learning&#x20;Sciences.</dcvalue>
      <dcvalue element="title" qualifier="none" language="en_US">Can We Rely on IRR? Testing the Assumptions of Inter-Rater Reliability</dcvalue>
      <dcvalue element="type" qualifier="none" language="en_US">Book&#x20;chapter</dcvalue>
      <dc:subject xml:lang="en">Innovative Design Approach</dc:subject>
<dc:subject xml:lang="en">Identity and Community</dc:subject>
<dc:subject xml:lang="en">Motivation</dc:subject>
<dc:subject xml:lang="en">Self-Regulated Learning</dc:subject>
    </dublin_core>